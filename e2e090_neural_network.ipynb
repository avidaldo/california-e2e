{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Neural Networks with PyTorch for Regression (California Housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "We will compare the performance of our neural network models with a\n",
    "previously trained RandomForest model that achieved an RMSE of approximately 41,000 dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "\n",
    "# Determine the device (GPU if available, otherwise CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    torch.cuda.manual_seed(RANDOM_STATE) # Initialize seed for GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "\n",
    "num_workers = os.cpu_count() // 2 # Number of workers. Half of the available cores (common heuristic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load_california import load_housing_data\n",
    "\n",
    "VAL_SIZE = 0.2\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = load_housing_data()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=VAL_SIZE, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "We have separated a validation set. For simplicity, we won't perform *cross-validation* since it is less common in *deep learning* because larger amounts of data are typically used for training, which increases computational cost. However, in this case, the dataset isn't that large and it would be feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "The necessary preprocessing functions defined in the [`utils/housing_preprocessing.py`](utils/housing_preprocessing.py) module are imported. The pipeline must be **fitted** *only* with the **training** data (`X_train`) to avoid information leakage. Then, it is used to **transform** the other sets.\n",
    "\n",
    "After preprocessing, we will convert the data from NumPy arrays to **PyTorch Tensors**, which are the fundamental data structure in PyTorch.\n",
    "\n",
    "For the number of clusters in geospatial similarity, we used the value that gave the best results in [the hyperparameter search](e2e081_hyperparameters2.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.housing_preprocessing import get_preprocessing_pipeline\n",
    "\n",
    "preprocessing_pipeline = get_preprocessing_pipeline(n_clusters=76)\n",
    "\n",
    "# Fit the pipeline ONLY with training data\n",
    "preprocessing_pipeline.fit(X_train)\n",
    "\n",
    "# Apply the pipeline to transform the input data\n",
    "X_train_processed = preprocessing_pipeline.transform(X_train)\n",
    "X_val_processed = preprocessing_pipeline.transform(X_val)\n",
    "X_test_processed = preprocessing_pipeline.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Feature normalization is essential for neural networks because bringing all attributes to comparable scales prevents those with larger magnitudes from dominating the gradient calculations, which accelerates the convergence of optimization algorithms and improves numerical stability by preventing overflow or \"dead\" gradients in the flat regions of activation functions. In this case, the preprocessing *pipeline* already standardizes the input variables.\n",
    "\n",
    "Likewise, normalizing the target variable in regression tasks helps the cost function operate in a reasonable range, facilitates more consistent update steps during learning, and can improve both the convergence speed and the final quality of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.housing_preprocessing import scale_target\n",
    "y_train_scaled_np, y_val_scaled_np, y_test_scaled_np, y_scaler = scale_target(y_train, y_val, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Defining the Evaluation Metric (and Loss Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The evaluation metric (RMSE) was previously defined. Therefore, for neural network models, we will use the `nn.MSELoss()` loss function. Minimizing MSE is equivalent to minimizing RMSE. We will use RMSE to evaluate the model with a more interpretable value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss() # For neural networks with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Baseline: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "We start with a Random Forest model using the best hyperparameters found previously; with these, an RMSE of 41,604 USD was achieved using *cross-validation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error # To skip a cell during execution\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=290,\n",
    "    max_depth=87,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=num_workers\n",
    ")\n",
    "rf_model.fit(X_train_processed, y_train)\n",
    "y_val_pred = rf_model.predict(X_val_processed)\n",
    "val_rmse = root_mean_squared_error(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Random Forest RMSE: {val_rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "For the validation set defined in this case, the Random Forest model achieves an RMSE of **44,711 USD. This will be our reference value** for comparison with the neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Comparison with Neural Network Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert NumPy data to PyTorch Tensors and move to GPU if available\n",
    "X_train_tensor = torch.tensor(X_train_processed.astype(np.float32)).to(device)\n",
    "X_val_tensor = torch.tensor(X_val_processed.astype(np.float32)).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_processed.astype(np.float32)).to(device)\n",
    "\n",
    "# Determine the number of input features for the network\n",
    "n_features = X_train_tensor.shape[1]\n",
    "print(f\"Number of input features: {n_features}\")\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train_scaled_np.astype(np.float32)).to(device)\n",
    "y_val_tensor = torch.tensor(y_val_scaled_np.astype(np.float32)).to(device)\n",
    "y_test_tensor = torch.tensor(y_test_scaled_np.astype(np.float32)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "We use the `TensorDataset` class to create a dataset from the feature and label tensors. Then, we use the `DataLoader` class to create a `DataLoader` from the dataset. The `DataLoader` allows us to iterate over the dataset in *batches*, shuffling the data in each *epoch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## Modelo 1: MLP Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(n_features, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(64, 32)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu1(self.layer1(x))\n",
    "        x = self.relu2(self.layer2(x))\n",
    "        x = self.relu3(self.layer3(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "    \n",
    "model1 = SimpleMLP(n_features).to(device)\n",
    "optimizer = optim.Adam(model1.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.training_utils import train_model_detailed, plot_metrics\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "metrics1 = train_model_detailed(\n",
    "    model=model1, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    y_scaler=y_scaler, \n",
    "    num_epochs=num_epochs, \n",
    "    device=device,\n",
    "    print_every=10 # Print metrics every 10 epochs\n",
    ")\n",
    "\n",
    "plot_metrics(metrics1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Model 2: Neural Network with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Model2(nn.Module):\n",
    "    def __init__(self, n_features):\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_features, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2),\n",
    "\n",
    "            nn.Linear(64, 1)  # single‚Äêvalue regression\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "model2 = Model2(n_features).to(device)\n",
    "optimizer = optim.Adam(model2.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics2 = train_model_detailed(\n",
    "    model=model2, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    y_scaler=y_scaler, \n",
    "    num_epochs=num_epochs, \n",
    "    device=device,\n",
    "    print_every=10 # Print metrics every 10 epochs\n",
    ")\n",
    "\n",
    "plot_metrics(metrics1, metrics2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "The new models are observed to not improve upon the reference RMSE.\n",
    "\n",
    "However, it should be noted that the Random Forest model underwent hyperparameter tuning with *cross-validation*, while the neural network model has not been tuned."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
