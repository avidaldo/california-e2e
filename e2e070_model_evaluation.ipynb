{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Preprocessing Pipeline\n",
    "\n",
    "The preprocessing pipeline developed in previous notebooks ([e2e050](e2e050_pipelines.ipynb), [e2e051](e2e051_custom_transformers.ipynb), [e2e060](e2e060_spatial_clustering.ipynb)) is imported from the shared module [`utils/housing_preprocessing.py`](utils/housing_preprocessing.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.housing_preprocessing import get_preprocessing_pipeline\n",
    "preprocessing = get_preprocessing_pipeline(n_clusters=10)  # Default value for initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "The data loading with stratified train/test split is imported from [`utils/load_california.py`](utils/load_california.py), as developed in [e2e025](e2e025_train_test.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load_california import load_housing_data\n",
    "X_train, X_test, y_train, y_test = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Complete Pipeline with a Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "lin_reg = make_pipeline(preprocessing, LinearRegression())\n",
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lin_reg.predict(X_test) # Make predictions with the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare some of the predicted results with their actual labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actual values:\", list(y_test.iloc[:10]))\n",
    "print(\"Predictions:\", list(y_pred[:10].round(-2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and see the percentage error in these predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_ratios = y_pred.round(-2) / y_test - 1\n",
    "print(\", \".join([f\"{100 * ratio:.1f}%\" for ratio in error_ratios]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but we can evaluate performance with the root mean squared error, as we had established:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "root_mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An error of $68,812 for predictions of house values with a median price of $206,856 doesn't seem very useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Another Model: Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\n",
    "tree_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree_reg.predict(X_test)\n",
    "root_mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the error measured on the test set is still too high to consider that we have a useful model (a house price estimate with an error of $68,000 is probably much worse than what a real estate professional would subjectively estimate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Data Leakage*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we have just made a common mistake: by using the test set to compare models (Linear Regression vs. Decision Tree), we have committed **Model Selection Leakage**â€”a form of [data leakage](e2e025_train_test.ipynb#Data-Leakage).\n",
    "\n",
    "Even though we never trained on test data directly, the test set performance influenced our model choice. This means:\n",
    "\n",
    "- The test error is now an **optimistically biased** estimate of true generalization performance\n",
    "- We have effectively \"overfit to the test set\"\n",
    "- The test set can no longer serve as an unbiased proxy for unseen data\n",
    "\n",
    "**The solution**: Keep the test set in a \"vault\" and use it **only once**, at the very end. For comparing models and tuning hyperparameters, we need a separate **validation set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To avoid improperly using the test set during model development, the dataset is usually split into three parts:\n",
    "\n",
    "- **Training**: To fit the model's **parameters**.\n",
    "- **Validation**: To tune **hyperparameters** and **compare different models**. This set allows measuring intermediate performance without compromising the test set.\n",
    "- **Test**: To evaluate the final model only once. It should remain untouched throughout the entire development process.\n",
    "\n",
    "The validation set is used to compare models and tune hyperparameters. The test set is reserved for evaluating the final performance of the chosen model that will be deployed in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Cross-validation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have split the data into two sets: training and test. However, in many cases, performance will vary depending on the sampling we have done. If we stop fixing the sampling seed (`random_state` parameter of the `train_test_split` function), we will get different results (although in this case they don't vary much for either model).\n",
    "\n",
    "A more efficient approach is **cross-validation**: instead of splitting the training set in two, it is divided into *k* sets (*folds*). Then the model is trained *k* times, each time leaving a different set as the validation set and the other *k-1* as the training set. The result is an array with *k* scores.\n",
    "\n",
    "For example, the following code performs training with 10 different samplings. The results will be similar to what we could obtain by running the code 10 times without fixing the random sampling seed. The trade-off is obvious: the computational cost is also multiplied by 10.\n",
    "\n",
    "This introduces the concept of the **validation set**. The validation set is used to compare models and **tune hyperparameters**, and it changes in each iteration of cross-validation. The test set is reserved for evaluating the final performance of the chosen model once it has been trained.\n",
    "\n",
    "[<img src=\"./img/cross-validation.png\" width=\"500\">](https://www.researchgate.net/figure/Train-test-cross-validation-split-methodology-used-in-this-paper-The-first-operation_fig2_340567535)\n",
    "\n",
    "[<img src=\"./img/cross-validation2.png\" width=\"700\">](https://www.statology.org/validation-set-vs-test-set/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "tree_rmses = -cross_val_score(estimator = tree_reg, \n",
    "                              X = X_train,\n",
    "                              y = y_train,\n",
    "                              scoring = \"neg_root_mean_squared_error\",\n",
    "                              cv = 10) # 10-fold cross-validation\n",
    "\n",
    "print(tree_rmses)\n",
    "pd.Series(tree_rmses).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scoring` parameter of the `cross_val_score` function expects a **utility function** (higher is better) rather than a **cost function** (lower is better), so the score is actually the negative of the RMSE. It is a negative value, so we need to flip the sign of the output to get the RMSE values. `cross_val_score` will seek to maximize the score, so by maximizing the negative of the RMSE, we minimize the RMSE.\n",
    "\n",
    "There are multiple string identifiers for evaluation metrics that can be used in `scoring`, which can be found in the [Scikit-Learn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter).\n",
    "\n",
    "We can see that we obtain a mean RMSE of $67,431 **on validation** with a standard deviation of $3,623. This provides more detailed information (and less dependent on the sampling we performed) about the model's performance.\n",
    "\n",
    "Now we have an evaluation metric for our decision tree model that we can compare with others without touching the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Another Model (*Random Forest*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Random Forest* is a model that trains multiple decision trees (***ensemble learning***) on random subsets of features and averages their predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = make_pipeline(preprocessing, RandomForestRegressor(random_state=42))\n",
    "forest_rmses = -cross_val_score(forest_reg, X_train, y_train, scoring = \"neg_root_mean_squared_error\", cv = 10)\n",
    "pd.Series(forest_rmses).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest achieves an improvement ($47,328 mean error) on the validation set compared to the simple decision tree.\n",
    "\n",
    "Although it is still a high error, it is the best model we have so far. Assuming we stick with it, we could finally train the chosen model on the entire training set and evaluate its performance on the test set we kept separate before putting it into production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = forest_reg.fit(X_train, y_train).predict(X_test)\n",
    "forest_rmse = root_mean_squared_error(y_test, y_pred)\n",
    "forest_rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wip-clase (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
