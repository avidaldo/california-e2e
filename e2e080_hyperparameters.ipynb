{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous model, we achieved an RMSE of $47,328 with a RandomForestRegressor model. Let's tune its hyperparameters to try to reduce this error further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Preprocessing Pipeline\n",
    "\n",
    "The preprocessing pipeline developed in previous notebooks is imported from the shared module [`utils/housing_preprocessing.py`](utils/housing_preprocessing.py). We use a low default value for `n_clusters` since we will be tuning this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from utils.housing_preprocessing import get_preprocessing_pipeline\n",
    "\n",
    "preprocessing = get_preprocessing_pipeline(n_clusters=10)  # Low default, will be tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing),\n",
    "    (\"random_forest\", RandomForestRegressor(random_state=42)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "The data loading with stratified train/test split is imported from [`utils/load_california.py`](utils/load_california.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load_california import load_housing_data\n",
    "X_train, X_test, y_train, y_test = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the names of the hyperparameters that can be tuned, you can use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in sorted(full_pipeline.get_params().keys()):\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Grid Search*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid the tedious process of manually modifying a model's hyperparameters until finding the ones that yield the best results, we can define all the hyperparameter values we want to test and program them to try all possible combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {'preprocessing__geo__n_clusters': [5, 8, 10], # number of clusters for the geo transformer\n",
    "     'random_forest__max_features': [4, 6, 8]}, # number of features to consider when looking for the best split\n",
    "    {'preprocessing__geo__n_clusters': [10, 15],\n",
    "     'random_forest__max_features': [6, 8, 10]},\n",
    "]\n",
    "grid_search = GridSearchCV(\n",
    "    estimator = full_pipeline,\n",
    "    param_grid = param_grid, \n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1\n",
    "    )\n",
    "\n",
    "_ = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```param_grid``` parameter is a list of dictionaries, each containing the hyperparameter values we want to test. In this case, we're first testing 3 values for the number of clusters and 3 for the number of features considered in each split. Then we're testing 2 values for the number of clusters and 3 for the number of features. In total, we're testing 3×3 + 2×3 = 15 hyperparameter combinations.\n",
    "\n",
    "Additionally, the ```n_jobs``` parameter allows parallelizing the hyperparameter search by indicating the number of processors to use; a value of -1 means all available processors will be used. This same parameter can be used in the RandomForestRegressor model to parallelize tree construction, but you need to be careful if doing both, since if you parallelize each hyperparameter search, which is itself a model execution, and that model in turn parallelizes tree construction, the total number of executions would multiply. The total n_jobs of RandomForestRegressor multiplied by the number of searches cannot exceed the number of physical cores on the machine. In general, it's better to parallelize the hyperparameter search rather than tree construction; therefore, in this case, we've chosen to leave RandomForestRegressor with the default value ```n_jobs=None```, which assigns 1 core per tree, and use the maximum for GridSearchCV with ```n_jobs=-1```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the best hyperparameters found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the best model has 15 clusters. Since this is the highest value tested, it would make sense to run new tests with larger values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It also returns the best estimator found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the result of each hyperparameter combination tested during the search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n",
    "\n",
    "# Select the columns we want to display\n",
    "cv_res = cv_res[[\"param_preprocessing__geo__n_clusters\",\n",
    "                 \"param_random_forest__max_features\", \"split0_test_score\",\n",
    "                 \"split1_test_score\", \"split2_test_score\", \"mean_test_score\"]]\n",
    "\n",
    "# Rename columns for simplicity\n",
    "score_cols = [\"split0\", \"split1\", \"split2\", \"mean_test_rmse\"]\n",
    "cv_res.columns = [\"n_clusters\", \"max_features\"] + score_cols\n",
    "# Clean up the score metric\n",
    "cv_res[score_cols] = -cv_res[score_cols].round().astype(np.int64)\n",
    "cv_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of testing all possible hyperparameter combinations, RandomizedSearchCV allows testing a specified number of random combinations. This provides certain advantages:\n",
    "\n",
    "- Computational efficiency: The \"curse of dimensionality\" makes Grid Search computationally infeasible very quickly. With more than 3 or 4 hyperparameters with a few options each, the total number of combinations to test explodes. Randomized Search allows setting a computational budget (number of iterations) independent of the number of hyperparameters, making it feasible for complex problems.\n",
    "\n",
    "- Effectiveness in high dimensions: For many objective functions (such as model performance), only a few hyperparameters have a significant impact. By randomly sampling combinations, there's a higher probability of testing diverse values in the important dimensions, while Grid Search wastes much effort systematically testing values in dimensions that barely affect the result.\n",
    "\n",
    "- Handling continuous parameters: Randomized Search naturally handles continuous parameters by sampling from a distribution (e.g., uniform, log-uniform). Grid Search requires discretizing the range, which is artificial and can easily miss the actual optimal value if it falls between grid points.\n",
    "\n",
    "For these reasons, RandomizedSearchCV is generally more efficient than GridSearchCV for high-dimensional problems, with many hyperparameters, or where we don't have a clear idea of the hyperparameter value ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(3,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_distribs = {\n",
    "    'preprocessing__geo__n_clusters': randint(low=3, high=50),\n",
    "    'random_forest__max_features': randint(low=2, high=20)\n",
    "    }\n",
    "\n",
    "rnd_search = RandomizedSearchCV(\n",
    "    full_pipeline,\n",
    "    param_distributions=param_distribs,\n",
    "    n_iter=10, # number of iterations\n",
    "    cv=3,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    "    )\n",
    "\n",
    "_ = rnd_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```scipy.stats.randint()``` returns an object containing the probability distribution of the discrete random variable. RandomizedSearchCV uses it to randomly sample hyperparameter values.\n",
    "\n",
    "The ```n_iter``` parameter is the number of iterations to perform. In this case, we're testing 10 hyperparameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame(rnd_search.cv_results_)\n",
    "cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n",
    "cv_res = cv_res[[\"param_preprocessing__geo__n_clusters\",\n",
    "                 \"param_random_forest__max_features\", \"split0_test_score\",\n",
    "                 \"split1_test_score\", \"split2_test_score\", \"mean_test_score\"]]\n",
    "score_cols = [\"split0\", \"split1\", \"split2\", \"mean_test_rmse\"]\n",
    "cv_res.columns = [\"n_clusters\", \"max_features\"] + score_cols\n",
    "cv_res[score_cols] = -cv_res[score_cols].round().astype(np.int64)\n",
    "cv_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've managed to improve our model by reducing the RMSE to $42,560 by defining 45 clusters and considering 9 features for each split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Final Model on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "final_predictions = rnd_search.best_estimator_.predict(X_test)\n",
    "\n",
    "final_rmse = root_mean_squared_error(y_test, final_predictions)\n",
    "print(final_rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
