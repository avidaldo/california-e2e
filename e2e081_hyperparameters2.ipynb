{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Preprocessing Pipeline\n",
    "\n",
    "The preprocessing pipeline developed in previous notebooks is imported from the shared module [`utils/housing_preprocessing.py`](utils/housing_preprocessing.py). We use a low default value for `n_clusters` since we will be tuning this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from utils.housing_preprocessing import get_preprocessing_pipeline\n",
    "\n",
    "preprocessing = get_preprocessing_pipeline(n_clusters=10)  # Low default, will be tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = Pipeline([\n",
    "    (\"preprocessing\", preprocessing),\n",
    "    (\"random_forest\", RandomForestRegressor(random_state=42, n_jobs=1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "The data loading with stratified train/test split is imported from [`utils/load_california.py`](utils/load_california.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.load_california import load_housing_data\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the preprocessing pipeline:\n",
    "\n",
    "| Hyperparameter      | Description                                                 |\n",
    "|---------------------|-------------------------------------------------------------|\n",
    "| `n_clusters`        | Number of clusters corresponding to geographic zones.   |\n",
    "| `gamma`             | Rate of decay for similarity with the centroid.        |\n",
    "| `strategy`          | Imputation strategy for missing values (default is mean).        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For RandomForestRegressor:\n",
    "\n",
    "| Hyperparameter      | Description |\n",
    "|---------------------|-------------|\n",
    "| `n_estimators`     | Number of trees in the forest. More trees can improve accuracy but increase computation time. |\n",
    "| `max_depth`        | Maximum depth of each tree. A low value may lead to *underfitting*, while a high value may lead to *overfitting*. |\n",
    "| `max_features`     | Number of *features* considered at each split. Can be an integer, a percentage, `\"sqrt\"` or `\"log2\"`. Fewer *features* can reduce variance (and thus *overfitting*). |\n",
    "| `min_samples_split` | Minimum number of samples required to split a node. Higher values reduce *overfitting*. |\n",
    "| `min_samples_leaf`  | Minimum number of samples in a leaf. Higher values smooth the prediction. |\n",
    "| `max_samples`      | Percentage of samples used in each tree. Useful for reducing *overfitting*. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Iteration\n",
    "\n",
    "Let's start with a preliminary randomized search using a broad range of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "param_dist = {\n",
    "    'preprocessing__geo__n_clusters': randint(low=3, high=200),\n",
    "    'random_forest__n_estimators': randint(100, 500),  # Any integer between 100 and 499\n",
    "    'random_forest__max_depth': randint(10, 110),      # Any integer between 10 and 109\n",
    "    'random_forest__min_samples_split': randint(2, 20),\n",
    "    'random_forest__min_samples_leaf': randint(1, 20),\n",
    "    'random_forest__max_features': ['sqrt', 'log2', None]\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(\n",
    "    estimator = full_pipeline, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=40, \n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1   # Use all CPU cores in parallel\n",
    "    )\n",
    "\n",
    "_ = rnd_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```%%time``` is a [Jupyter magic command](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-time) that measures the execution time of the cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the results of the best models found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame(rnd_search.cv_results_)\n",
    "cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n",
    "\n",
    "cv_res = cv_res[['param_preprocessing__geo__n_clusters',\n",
    "                 'param_random_forest__n_estimators',\n",
    "                 'param_random_forest__max_depth',\n",
    "                 'param_random_forest__min_samples_split',\n",
    "                 'param_random_forest__min_samples_leaf',\n",
    "                 'param_random_forest__max_features',\n",
    "                 \"mean_test_score\"]]\n",
    "cv_res.columns = [\"n_clusters\", \"n_estimators\", \"max_depth\", \"min_samples_split\", \"min_samples_leaf\", \"max_features\", \"mean_test_score\"]\n",
    "\n",
    "cv_res[\"mean_test_score\"] = -cv_res[\"mean_test_score\"].round().astype(np.int64)\n",
    "cv_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform successive iterations by fixing *features* where all the best results have converged to a single value, and defining a narrower dictionary of test values centered around the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "full_pipeline.set_params(random_forest__max_features=\"sqrt\") # Fix the value of max_features, which has converged to \"sqrt\"\n",
    "\n",
    "param_dist = {\n",
    "    'preprocessing__geo__n_clusters': randint(low=55, high=150),\n",
    "    'random_forest__n_estimators': randint(200, 300),\n",
    "    'random_forest__max_depth': randint(44, 97),\n",
    "    'random_forest__min_samples_split': randint(2, 14),\n",
    "    'random_forest__min_samples_leaf': randint(1, 5),\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(\n",
    "    estimator = full_pipeline, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=40, \n",
    "    cv=5,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1   # Use all CPU cores in parallel\n",
    "    )\n",
    "\n",
    "_ = rnd_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_res = pd.DataFrame(rnd_search.cv_results_)\n",
    "cv_res.sort_values(by=\"mean_test_score\", ascending=False, inplace=True)\n",
    "\n",
    "cv_res = cv_res[['param_preprocessing__geo__n_clusters',\n",
    "                 'param_random_forest__n_estimators',\n",
    "                 'param_random_forest__max_depth',\n",
    "                 'param_random_forest__min_samples_split',\n",
    "                 'param_random_forest__min_samples_leaf',\n",
    "                 \"mean_test_score\"]]\n",
    "cv_res.columns = [\"n_clusters\", \"n_estimators\", \"max_depth\", \"min_samples_split\", \"min_samples_leaf\", \"mean_test_score\"]\n",
    "\n",
    "cv_res[\"mean_test_score\"] = -cv_res[\"mean_test_score\"].round().astype(np.int64)\n",
    "cv_res.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wip-clase (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
