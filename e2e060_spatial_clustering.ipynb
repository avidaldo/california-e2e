{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Latitude and Longitude in the California Housing Prices Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing = pd.read_csv(\"./data/housing.csv\") \n",
    "train_set, test_set = train_test_split(housing, test_size=0.2,\n",
    "    stratify=pd.cut(housing[\"median_income\"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5]),\n",
    "    random_state=42\n",
    "    )\n",
    "\n",
    "X_train = train_set.drop(\"median_house_value\", axis=1)\n",
    "y_train = train_set[\"median_house_value\"].copy() # Store the target variable (labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Geographic Coordinates in the *Dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geographic coordinates present unique challenges for machine learning models:\n",
    "\n",
    "1. **Non-linear spatial relationships**: Housing prices don't increase or decrease monotonically with latitude or longitude. A district at latitude 37.5° isn't inherently \"better\" than one at 36.5°—the relationship is highly non-linear and location-dependent.\n",
    "\n",
    "2. **Interaction effects**: Latitude and longitude only become meaningful *together*. The point (37.7749, -122.4194) is San Francisco, but neither coordinate alone carries that information. Linear models cannot capture this interaction without explicit feature engineering.\n",
    "\n",
    "3. **Distance matters, not absolute position**: What typically matters is *proximity* to important locations (coast, city centers, employment hubs), not the raw coordinates themselves.\n",
    "\n",
    "4. **Wrap-around and scale issues**: Longitude wraps at ±180°, and the actual ground distance represented by 1° varies with latitude.\n",
    "\n",
    "To address these challenges, we can transform coordinates into more useful features:\n",
    "- **Cluster membership**: Group districts into geographic regions\n",
    "- **Distance-based features**: Measure proximity to cluster centers or points of interest\n",
    "- **Similarity scores**: Use kernel functions to capture \"nearness\" to important locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means for Geographic Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "K-means is a *clustering* algorithm (unsupervised learning) that we could use to group districts into similar geographic regions. To do this, we'll use the geographic coordinates of the districts and group them into a fixed number of geographic regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Apply K-means with k=6 (for example, to divide California into 6 regions)\n",
    "kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "X_train['region_cluster'] = kmeans.fit_predict(X_train[['latitude', 'longitude']])\n",
    "X_train[['latitude', 'longitude', 'region_cluster']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(X_train['longitude'], X_train['latitude'], \n",
    "            c=X_train['region_cluster'], cmap='viridis', \n",
    "            s=5, alpha=0.7)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 1], kmeans.cluster_centers_[:, 0], \n",
    "            c='red', s=50, marker='x')\n",
    "plt.title('Clustering of California Districts Using K-means')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could create a custom transformer for this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class RegionClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=6, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=random_state)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fits the KMeans model using the 'latitude' and 'longitude' columns.\n",
    "        X is assumed to be a pandas DataFrame containing these columns.\n",
    "        \"\"\"\n",
    "        self.kmeans.fit(X[['latitude', 'longitude']])\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transforms the DataFrame X by adding a 'region_cluster' column containing\n",
    "        the cluster prediction for each record.\n",
    "        \"\"\"\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed['region_cluster'] = self.kmeans.predict(X_transformed[['latitude', 'longitude']])\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = RegionClusterTransformer(n_clusters=6, random_state=42)\n",
    "transformer.fit(X_train)\n",
    "housing_transformed = transformer.transform(X_train)\n",
    "housing_transformed[['latitude', 'longitude', 'region_cluster']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Distance to Centroids as a Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, housing prices are not only related by the cluster of districts they belong to, but districts closer to the cluster center are likely to have higher prices. Therefore, it would be useful to create a feature that captures the \"proximity\" to the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DistanceClusterTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom transformer for grouping geographic locations using K-means.\n",
    "    Expects a DataFrame with at least two columns: 'latitude' and 'longitude'.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters=8, random_state=42):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.random_state = random_state\n",
    "        self.kmeans_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # X is expected to be a DataFrame or array with 'latitude' and 'longitude' columns\n",
    "        # Convert to array if X is a DataFrame:\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            coords = X[['latitude', 'longitude']].values\n",
    "        else:\n",
    "            coords = X[:, :2]  # assuming the first two columns are lat/lon\n",
    "        self.kmeans_ = KMeans(n_clusters=self.n_clusters, random_state=self.random_state)\n",
    "        self.kmeans_.fit(coords)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Ensure X is a DataFrame for easier column handling\n",
    "        X_trans = X.copy() if isinstance(X, pd.DataFrame) else pd.DataFrame(X, columns=['latitude', 'longitude'])\n",
    "        coords = X_trans[['latitude', 'longitude']].values\n",
    "        \n",
    "        # Predict the cluster label\n",
    "        cluster_labels = self.kmeans_.predict(coords)\n",
    "        X_trans['geo_cluster'] = cluster_labels\n",
    "        \n",
    "        # Calculate the distance from each point to its assigned centroid\n",
    "        distances = []\n",
    "        for i, point in enumerate(coords):\n",
    "            centroid = self.kmeans_.cluster_centers_[cluster_labels[i]]\n",
    "            distance = np.linalg.norm(point - centroid)\n",
    "            distances.append(distance)\n",
    "        X_trans['distance_to_centroid'] = distances\n",
    "        \n",
    "        return X_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoClusterTransformer = DistanceClusterTransformer(n_clusters=5)\n",
    "geoClusterTransformer.fit(X_train[['longitude', 'latitude']])\n",
    "housing_geo_clusters = geoClusterTransformer.transform(X_train[['longitude', 'latitude']])\n",
    "housing_geo_clusters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity with rbf_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rbf_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `rbf_kernel` function calculates the similarity between two datasets through the *radial basis function* (RBF), returning a value between 0 and 1.\n",
    "For example, if we assumed that properties around 35 years old increase in value due to some particularity, we could measure the distance of each one to those that are 35 years old and assign a similarity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "age_simil_35 = rbf_kernel(X_train[[\"housing_median_age\"]], [[35]], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ages = np.linspace(X_train[\"housing_median_age\"].min(),\n",
    "                   X_train[\"housing_median_age\"].max(),\n",
    "                   500).reshape(-1, 1)\n",
    "gamma1 = 0.1\n",
    "gamma2 = 0.03\n",
    "rbf1 = rbf_kernel(ages, [[35]], gamma=gamma1)\n",
    "rbf2 = rbf_kernel(ages, [[35]], gamma=gamma2)\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.set_xlabel(\"Housing median age\")\n",
    "ax1.set_ylabel(\"Number of districts\")\n",
    "ax1.hist(X_train[\"housing_median_age\"], bins=50)\n",
    "\n",
    "ax2 = ax1.twinx()  # create a twin axis that shares the same x-axis\n",
    "color = \"blue\"\n",
    "ax2.plot(ages, rbf1, color=color, label=\"gamma = 0.10\")\n",
    "ax2.plot(ages, rbf2, color=color, label=\"gamma = 0.03\", linestyle=\"--\")\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_ylabel(\"Age similarity\", color=color)\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the `gamma` value allows adjusting the similarity between the data, widening or narrowing the RBF function, which is a normal (Gaussian) distribution centered on the reference value. On the right, we can see the scale from 0 to 1 of similarity that records close to the reference value will return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Similarity to Centroids with rbf_kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of distance, we can use rbf_kernel to measure the proximity of each district to the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "class ClusterSimilarity(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_clusters=10, gamma=1.0, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.gamma = gamma\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y=None, sample_weight=None):\n",
    "        self.kmeans_ = KMeans(self.n_clusters, n_init=10,\n",
    "                              random_state=self.random_state)\n",
    "        self.kmeans_.fit(X, sample_weight=sample_weight)\n",
    "        return self  # always return self!\n",
    "\n",
    "    def transform(self, X):\n",
    "        return rbf_kernel(X, self.kmeans_.cluster_centers_, gamma=self.gamma)\n",
    "    \n",
    "    def get_feature_names_out(self, names=None):\n",
    "        return [f\"Cluster {i} similarity\" for i in range(self.n_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_simil = ClusterSimilarity(n_clusters=10, gamma=1., random_state=42)\n",
    "similarities = cluster_simil.fit_transform(X_train[[\"latitude\", \"longitude\"]],\n",
    "                                           sample_weight=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_renamed = X_train.rename(columns={\n",
    "    \"latitude\": \"Latitude\", \"longitude\": \"Longitude\",\n",
    "    \"population\": \"Population\",\n",
    "    \"median_house_value\": \"Median house value (ᴜsᴅ)\"})\n",
    "housing_renamed[\"Max cluster similarity\"] = similarities.max(axis=1)\n",
    "\n",
    "housing_renamed.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", grid=True,\n",
    "                     s=housing_renamed[\"Population\"] / 100, label=\"Population\",\n",
    "                     c=\"Max cluster similarity\",\n",
    "                     cmap=\"jet\", colorbar=True,\n",
    "                     legend=True, sharex=False, figsize=(10, 7))\n",
    "plt.plot(cluster_simil.kmeans_.cluster_centers_[:, 1],\n",
    "         cluster_simil.kmeans_.cluster_centers_[:, 0],\n",
    "         linestyle=\"\", color=\"black\", marker=\"X\", markersize=20,\n",
    "         label=\"Cluster centers\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Preprocessing Pipeline\n",
    "\n",
    "At this point, we have developed all the components needed for the complete preprocessing pipeline:\n",
    "\n",
    "- **Basic pipelines** ([e2e050](e2e050_pipelines.ipynb)): `SimpleImputer`, `StandardScaler`, `OneHotEncoder`, and `ColumnTransformer`\n",
    "- **Custom transformers** ([e2e051](e2e051_custom_transformers.ipynb)): `FunctionTransformer` for feature ratios and logarithmic transformations\n",
    "- **Geospatial features** (this notebook): `ClusterSimilarity` transformer using K-means and RBF kernel\n",
    "\n",
    "These components are combined into a reusable preprocessing module at [`utils/housing_preprocessing.py`](utils/housing_preprocessing.py), which is imported by subsequent notebooks for model training and evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wip-clase (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
