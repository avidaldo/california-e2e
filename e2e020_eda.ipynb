{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "**Exploratory Data Analysis (EDA)** is performed on the full dataset *before* splitting into training and test sets. This allows us to understand the data's structure, distributions, and relationships, which informs decisions about preprocessing and feature engineering.\n",
    "\n",
    "> ⚠️ **Data snooping warning**: While EDA is performed on the full dataset, it's crucial that any *decisions* derived from this analysis (such as choosing preprocessing strategies or creating new features) are only *fitted* on the training data after the split. The goal of EDA is to understand the data, not to optimize for specific values that might leak information from what will become the test set. For example, we can observe that `median_income` is highly correlated with the target here, but when we later perform stratified sampling or fit scalers, we must use only the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "housing = pd.read_csv(\"./data/housing.csv\") # Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geographic data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a scatter plot of all districts to visualize the geographic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=0.2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of each record on its geographic position (latitude and longitude), with a color scale representing the median house value and circle size representing the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True,\n",
    "             s=housing[\"population\"] / 100, # circle radius represents district population\n",
    "             label=\"population\", # label for the legend\n",
    "             c=\"median_house_value\", # point color represents median house value\n",
    "             cmap=\"jet\", # use the \"jet\" color palette\n",
    "             colorbar=True,\n",
    "             legend=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positioning the same on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\",\n",
    "             s=housing[\"population\"] / 100, # circle size proportional to population\n",
    "             label=\"Population\", \n",
    "             c=\"median_house_value\", cmap=\"jet\", colorbar=True,\n",
    "             legend=True)\n",
    "\n",
    "california_img = plt.imread(\"./img/california.png\")\n",
    "axis = -124.55, -113.95, 32.45, 42.05\n",
    "plt.axis(axis)\n",
    "plt.imshow(california_img, extent=axis)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the 'ocean_proximity' values with colors geographically (will be useful when we preprocess this categorical variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.scatterplot(x=\"longitude\", y=\"latitude\", hue=\"ocean_proximity\", data=housing)\n",
    "axis = -124.55, -113.95, 32.45, 42.05 # longitude and latitude limits of the image\n",
    "plt.imshow(PIL.Image.open(\"./img/california.png\", mode='r'), extent=axis)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `corr()` method of Pandas calculates the **Pearson correlation coefficient** (PCC) between each pair of attributes. This coefficient ranges from -1 to 1 indicating the degree of linear correlation between two variables. When it's close to 1, it means there's a strong positive correlation (variables tend to increase together). When the coefficient is close to -1, it means there's a strong negative correlation (one variable decreases when the other increases). Finally, coefficients close to 0 mean there's no linear correlation.\n",
    "We use the argument `numeric_only=True` to calculate correlation only between numeric variables, ignoring the categorical variable 'ocean_proximity'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr(numeric_only=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having this correlation matrix, let's see in an ordered way which variables correlate most with the **target variable**, 'median_house_value'. The `sort_values()` method of Pandas allows us to sort the values of a series; additionally, we can use the `key` parameter to pass a function that will be applied to each value before sorting. In this case, we want to sort by absolute value, since a strong negative correlation is just as interesting as a positive one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False, key=np.abs) # sort values by correlation with respect to absolute value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the variable that correlates most with 'median_house_value' is 'median_income', with a coefficient of 0.69. It's therefore the ***feature*** that most determines housing prices.\n",
    "\n",
    "We can also see the correlations with a scatter plot of the variables most correlated with 'median_house_value', using the `scatter_matrix` function from Pandas. On the diagonal we see that it places their histograms (as seen at the beginning) by default, since comparing a variable with itself would only give a straight line without useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(housing[attributes], figsize=(12, 8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we see how the clearest correlation is with 'median_income', the more it increases, the more the median house value increases. We can also see that the limit of 500,000 USD we mentioned before is clearly visible in the graph, as a horizontal line at the top of the y axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.1, grid=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers and capped values\n",
    "\n",
    "As we observed in the [framing notebook](e2e010_framing.ipynb), some variables in this dataset have been **capped**: values exceeding a certain threshold have been replaced with that threshold value. This is evident in the histograms of `housing_median_age` (capped at 52) and `median_house_value` (capped at 500,001 USD), where we see a tall bar at the maximum value.\n",
    "\n",
    "This is one of several techniques for handling **outliers**—data points that lie far from the bulk of the distribution. Common approaches include:\n",
    "\n",
    "- **Capping** (or **clipping**): Replacing values beyond a fixed threshold with that threshold. This is what was applied to this dataset. For example, all house values above 500,000 USD were set to 500,001 USD. This preserves the number of observations but loses information about the magnitude of extreme values.\n",
    "\n",
    "- **Winsorizing**: Similar to capping, but thresholds are defined as percentiles (e.g., the 1st and 99th percentiles) rather than fixed values. This is more statistically principled when the appropriate threshold is unknown.\n",
    "\n",
    "- **Truncation** (or **trimming**): Removing observations beyond the threshold entirely. Unlike capping, this reduces the sample size.\n",
    "\n",
    "These techniques matter because outliers can disproportionately affect:\n",
    "- **Mean imputation**: The mean is sensitive to extreme values, making median imputation preferable when outliers are present (see [missing values notebook](e2e041_missing.ipynb)).\n",
    "- **Min-Max scaling**: A single extreme value can compress the entire range of \"normal\" values into a small interval (see [scaling notebook](e2e043_scaling.ipynb)).\n",
    "- **Model training**: Many algorithms (especially those based on distances or gradients) can be distorted by extreme values.\n",
    "\n",
    "In this dataset, the capping was applied *before* we received the data. When working with raw data, the decision of whether and how to handle outliers should be made carefully, considering whether extreme values represent errors, rare but valid observations, or genuinely different populations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The insights gained from EDA—particularly correlations between variables—naturally lead to **feature engineering**: creating new variables that may be more predictive than the raw features. This is covered in the [Feature Engineering notebook](e2e030_feature_engineering.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wip-clase (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
