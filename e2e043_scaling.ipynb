{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Preprocessing: feature scaling"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "This notebook is an adaptation of the [original by *Aurélien Gerón*](https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb), from his book: [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 3rd Edition. Aurélien Géron](https://www.oreilly.com/library/view/hands-on-machine-learning/9781098125967/)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Previous steps"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nhousing = pd.read_csv(\"./data/housing.csv\") \n\n# Generation of training and test sets through stratified sampling by median income\ntrain_set, test_set = train_test_split(housing, test_size=0.2,\n    stratify=pd.cut(housing[\"median_income\"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5]),\n    random_state=42\n    )\n\nhousing = train_set.drop(\"median_house_value\", axis=1) # Remove the dependent variable column\nhousing_labels = train_set[\"median_house_value\"].copy() # Save the dependent variable (labels)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Scaling, normalization and standardization"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Most *Machine Learning* algorithms don't work well when *features* have very different scales. For example, many **classifiers** calculate the distance between two points using Euclidean distance. If one of the features has much larger values than the others, the distance will be dominated by this feature. For example, in our *dataset*, the range of 'median_income' goes from 0 to 15, while the range of 'total_rooms' goes from 6 to 39,320.\n\nTo avoid this, it's common to scale the *features*.\n\nThe terminology can be confusing at this point. In general, **normalization** refers to changing the scale of data to fit a specific range, while **standardization** refers to changing the distribution of data to have a mean of 0 and a standard deviation of 1. In both cases, they are linear transformations that don't change the shape of the data distribution. In statistics there's usually a clear distinction between both terms, but in deep learning and computer vision, the terminology can be less consistent and it's common to use \"normalization\" to refer to standardization.\n\n<!-- TODO: https://en.wikipedia.org/wiki/Normalization_(machine_learning) -->"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>longitude</th>\n",
       "      <td>16512.0</td>\n",
       "      <td>-119.575635</td>\n",
       "      <td>2.001828</td>\n",
       "      <td>-124.3500</td>\n",
       "      <td>-121.80000</td>\n",
       "      <td>-118.51000</td>\n",
       "      <td>-118.010000</td>\n",
       "      <td>-114.3100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>latitude</th>\n",
       "      <td>16512.0</td>\n",
       "      <td>35.639314</td>\n",
       "      <td>2.137963</td>\n",
       "      <td>32.5400</td>\n",
       "      <td>33.94000</td>\n",
       "      <td>34.26000</td>\n",
       "      <td>37.720000</td>\n",
       "      <td>41.9500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>housing_median_age</th>\n",
       "      <td>16512.0</td>\n",
       "      <td>28.653404</td>\n",
       "      <td>12.574819</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>18.00000</td>\n",
       "      <td>29.00000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>52.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_rooms</th>\n",
       "      <td>16512.0</td>\n",
       "      <td>2622.539789</td>\n",
       "      <td>2138.417080</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>1443.00000</td>\n",
       "      <td>2119.00000</td>\n",
       "      <td>3141.000000</td>\n",
       "      <td>39320.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_bedrooms</th>\n",
       "      <td>16354.0</td>\n",
       "      <td>534.914639</td>\n",
       "      <td>412.665649</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>295.00000</td>\n",
       "      <td>433.00000</td>\n",
       "      <td>644.000000</td>\n",
       "      <td>6210.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>population</th>\n",
       "      <td>16512.0</td>\n",
       "      <td>1419.687379</td>\n",
       "      <td>1115.663036</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>784.00000</td>\n",
       "      <td>1164.00000</td>\n",
       "      <td>1719.000000</td>\n",
       "      <td>35682.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>households</th>\n",
       "      <td>16512.0</td>\n",
       "      <td>497.011810</td>\n",
       "      <td>375.696156</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>279.00000</td>\n",
       "      <td>408.00000</td>\n",
       "      <td>602.000000</td>\n",
       "      <td>5358.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median_income</th>\n",
       "      <td>16512.0</td>\n",
       "      <td>3.875884</td>\n",
       "      <td>1.904931</td>\n",
       "      <td>0.4999</td>\n",
       "      <td>2.56695</td>\n",
       "      <td>3.54155</td>\n",
       "      <td>4.745325</td>\n",
       "      <td>15.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count         mean          std       min         25%  \\\n",
       "longitude           16512.0  -119.575635     2.001828 -124.3500  -121.80000   \n",
       "latitude            16512.0    35.639314     2.137963   32.5400    33.94000   \n",
       "housing_median_age  16512.0    28.653404    12.574819    1.0000    18.00000   \n",
       "total_rooms         16512.0  2622.539789  2138.417080    6.0000  1443.00000   \n",
       "total_bedrooms      16354.0   534.914639   412.665649    2.0000   295.00000   \n",
       "population          16512.0  1419.687379  1115.663036    3.0000   784.00000   \n",
       "households          16512.0   497.011810   375.696156    2.0000   279.00000   \n",
       "median_income       16512.0     3.875884     1.904931    0.4999     2.56695   \n",
       "\n",
       "                           50%          75%         max  \n",
       "longitude           -118.51000  -118.010000   -114.3100  \n",
       "latitude              34.26000    37.720000     41.9500  \n",
       "housing_median_age    29.00000    37.000000     52.0000  \n",
       "total_rooms         2119.00000  3141.000000  39320.0000  \n",
       "total_bedrooms       433.00000   644.000000   6210.0000  \n",
       "population          1164.00000  1719.000000  35682.0000  \n",
       "households           408.00000   602.000000   5358.0000  \n",
       "median_income          3.54155     4.745325     15.0001  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## MinMaxScaler"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "The most common normalization is **min-max normalization** or ***min-max scaling***. **Min-max normalization** is the simplest: values are scaled and shifted so that they end up in the range between a minimum value and a maximum value. Normally it will be between 0 and 1, although they can be others (neural networks usually work better with *inputs* with mean 0, so sometimes the range -1 to 1 is used). Scikit-Learn provides a `MinMaxScaler` class for this.\n\n$$ X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}} $$"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Min-Max normalization is very sensitive to *outliers*, since a single very large value can completely change the scale of the data. In a situation where all data is between 20 and 30 but a single value of 100 appears, the maximum becomes 100, shifting all other values to a very low range. In general, Min-Max normalization should only be used if we're sure that the *outliers* are not errors."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## StandardScaler"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "On the other hand, **Z-score standardization** (***standard score***) is different: first it subtracts the mean (so it becomes 0), and then divides by the **standard deviation** so that the resulting distribution has standard deviation 1. Unlike min-max scaling, standardization doesn't limit values to a specific range, but this also has the advantage of being much less sensitive to outliers. Scikit-Learn provides a `StandardScaler` class for this.\n\n$$ X_{std} = \\frac{X - \\mu}{\\sigma} $$"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "housing_num_std_scaled = std_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Many ML models work better by standardizing input *features* and it's a common and systematic practice in most cases (except for tree-based models). Scaling the *target* is less common, but can be useful in some cases, particularly for gradient-based models (such as neural networks) or distance-based models (such as KNN or SVM regressions).\n\nFor example, we could apply StandardScaler again to the labels:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "target_scaler = StandardScaler()\nscaled_labels = target_scaler.fit_transform(housing_labels.to_frame()) # convert the target to a dataframe (fit_transform expects 2D)\nprint(type(housing_labels)) # Since it's a single column, the labels were previously stored in a Series object\nscaled_labels"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Scaling target variables and subsequent inversion"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "If we transform the target variable in any way, our model's output will also return transformed predictions. If we want the predictions to be on the original scale, we'll need to invert the transformation. Many of Scikit-Learn's transformers have an `inverse_transform()` method, which makes it easy to calculate the inverse of their transformations.\n<!-- TODO: Categorical transformers don't have it since we'd then be in a classification problem. // Not all are invertible? -->\n\nTo give a simplified example, we're going to train a **simple linear regression with the most correlated predictor** (median_income) and the labels we just scaled. Then we'll test its predictions with the test set and undo the transformation."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.linear_model import LinearRegression\n\nmodel = LinearRegression()\nmodel.fit(housing[[\"median_income\"]], scaled_labels) # train the model with scaled independent variables\n\nsome_new_data = housing[[\"median_income\"]].iloc[:5]  # for simplicity, we simulate new inputs to predict by taking 5 rows (we haven't preprocessed the test set)\n\nscaled_predictions = model.predict(some_new_data)\ntarget_scaler.inverse_transform(scaled_predictions) # Undo the transformation to get predictions on the original scale"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "This process can be simplified using the `TransformedTargetRegressor` class from Scikit-Learn, which allows training a model with transformed labels and undoing the transformation automatically."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.compose import TransformedTargetRegressor\n\nmodel = TransformedTargetRegressor(regressor = LinearRegression(),\n                                   transformer = StandardScaler()) # dependent variable transformer\nmodel.fit(housing[[\"median_income\"]], housing_labels)\nmodel.predict(some_new_data)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Heavy-tailed distributions"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Heavy-tailed distributions are characterized by a long tail towards one side, which means they contain some very large values that can disproportionately influence the model. The logarithmic transformation compresses this long tail, reducing the range of values and making the distribution more normal. This helps decrease the impact of outliers, which leads to more stable and robust models. After applying the logarithm, scaling is still important to bring the transformed data to a common range, ensuring that all features contribute equally to the learning process and further improving model performance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from matplotlib import pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\nfig, axs = plt.subplots(2, 2, figsize=(10, 8))\n\n# Original distribution\nhousing[\"population\"].hist(ax=axs[0, 0], bins=50)\naxs[0, 0].set_xlabel(\"Population\")\naxs[0, 0].set_ylabel(\"Number of districts\")\n\n# Log transformation\nlog_pop = housing[\"population\"].apply(np.log)\nlog_pop.hist(ax=axs[0, 1], bins=50)\naxs[0, 1].set_xlabel(\"Log of population\")\naxs[0, 1].set_ylabel(\"Number of districts\")\n\n# Standard scaling\nscaler = StandardScaler()\nscaled_pop = scaler.fit_transform(housing[\"population\"].values.reshape(-1, 1)).flatten()\naxs[1, 0].hist(scaled_pop, bins=50)\naxs[1, 0].set_xlabel(\"Scaled population\")\naxs[1, 0].set_ylabel(\"Number of districts\")\n\n# Log transformation + standard scaling\nscaled_log_pop = scaler.fit_transform(log_pop.values.reshape(-1, 1)).flatten()\naxs[1, 1].hist(scaled_log_pop, bins=50)\naxs[1, 1].set_xlabel(\"Scaled log of population\")\naxs[1, 1].set_ylabel(\"Number of districts\")\n\nplt.tight_layout()\nplt.show()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}